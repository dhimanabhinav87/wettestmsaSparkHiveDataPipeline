# wettestmsaSparkHiveDataPipeline
This is a data pipe line sample implementation in Cloudera Hadoop using Spark, Hive, Shell and HDFS.

1) This code sorts all metropolitan statistical areas (MSAs) in the United States by wettest population during May of 2015. The population wetness of an MSA is calculated as the number of people in the MSA times the amount of rain received. For the purposes of this exercise,We  assumed that all people remain inside between the hours of 12 AM and 7 AM local time and so rainfall during these hours does not count. 

2) These are the files included in addtion to the scala class that you will need to use compile this code. 
 city_state_msa_fixed.txt : This is the City_State_MSA data assumed to be present at Edge node
 cbsa-est2016-alldata_cleaned.csv : This is the Population and MSA data assumed to be present at Edge node
 201505precip.txt : Please download this from link below
 station.txt : Please download this from link below
 Source of these data sets :  http://www.ncdc.noaa.gov/orders/qclcd/QCLCD201505.zip	

3) copy_file.sh : Shell script to copy the files from Edge node to HDFS
4) hive_action_final : Hive SQL query to create tables and load table generated by Spark
5) final_project_workflow.sh : Defines the workflow and executes the full data pipeline.Please change these according to your directory structure.
6) param_conf :Parameters for spark-submit for the locations of data file. Please change these according to your directory structure.Ignore STATION_LOCAL_PATH variable as this needs to be passed with spark-submit arguments due to current implementation.
7) pom.xml: Dependency file for scala spark.

 Thank You! 
 Abhinav Dhiman
